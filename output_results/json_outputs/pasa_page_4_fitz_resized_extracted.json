{
    "source_document": "output_results/resized_images/pasa_page_4_fitz_resized.png",
    "source_page_number": 0,
    "processing_timestamp_utc": "2025-11-17T21:37:13.286918+00:00",
    "status": "Layout extraction successful (Docker Inference)",
    "content": [
        {
            "extraction_id": 0,
            "metadata": {
                "source_page": 0,
                "type": "Section-header",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.5953,
                    "xmax": 0.3282992106860959,
                    "ymax": 0.607
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "### 3.2 RealScholarQuery"
            }
        },
        {
            "extraction_id": 1,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.618,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.7742
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "To evaluate PaSa in more realistic scenarios, we constructed RealScholarQuery, a test dataset consisting of 50 real-world research queries. After launching the demo of PaSa, we invited several AI researchers to use the system. From the queries they provided, we randomly sampled a subset of queries and manually filtered out overly broad topics (e.g., \"multi-modal large language models,\" \"video generation\"). Ultimately, we collected 50 fine-grained and realistic queries."
            }
        },
        {
            "extraction_id": 2,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.7805,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.9203
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "For each query, we first manually gathered relevant papers. Subsequently, we used multiple methods to retrieve additional papers, including PaSa, Google, Google Scholar, ChatGPT (search-enabled GPT-4o), and Google paired with GPT-4o for paraphrased queries. The results from these methods were aggregated into a pool of candidate papers. Finally, professional annotators reviewed all candidate papers for each query, selecting those that<tbc>"
            }
        },
        {
            "extraction_id": 3,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.393,
                    "xmax": 0.8364075288403158,
                    "ymax": 0.4688
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "<tbc>met the specific requirements of the query to create the final set of relevant papers. The query date of all instances in RealScholarQuery is 2024-10-01.  12 in Appendix D provides examples from RealScholarQuery."
            }
        },
        {
            "extraction_id": 4,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.4789,
                    "xmax": 0.8364075288403158,
                    "ymax": 0.5711
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "The annotators included professors from the Department of Computer Science at a top-tier university in China. On average, each query required the annotators to review 76 candidate papers. Given the high cost of the annotations, we completed this process for only 50 instances."
            }
        },
        {
            "extraction_id": 5,
            "metadata": {
                "source_page": 0,
                "type": "Section-header",
                "bbox": {
                    "xmin": 0.512010686095932,
                    "ymin": 0.6062,
                    "xmax": 0.6370865816636309,
                    "ymax": 0.6188
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "## 4 Methodology"
            }
        },
        {
            "extraction_id": 6,
            "metadata": {
                "source_page": 0,
                "type": "Section-header",
                "bbox": {
                    "xmin": 0.512010686095932,
                    "ymin": 0.6453,
                    "xmax": 0.6136723740133576,
                    "ymax": 0.6547
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "### 4.1 Overview"
            }
        },
        {
            "extraction_id": 7,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.6781,
                    "xmax": 0.8364075288403158,
                    "ymax": 0.8508
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "As illustrated in Figure 1, the PaSa system consists of two LLM agents: Crawler and Selector. The crawler reads the user\u2019s query, generates multiple search queries, and retrieves relevant papers. The retrieved papers are added to a _paper queue_. The Crawler further processes each paper in the paper queue to identify key citations worth exploring further, appending any newly relevant papers to the paper list. The selector conducts a thorough review of each paper in the paper list to assess whether it fulfills the user\u2019s query requirements."
            }
        },
        {
            "extraction_id": 8,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.512010686095932,
                    "ymin": 0.8609,
                    "xmax": 0.8364075288403158,
                    "ymax": 0.9172
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "In summary, the Crawler is designed to maximize the recall of relevant papers, whereas the Selector emphasizes precision in identifying papers that meet the user\u2019s needs."
            }
        },
        {
            "extraction_id": 9,
            "metadata": {
                "source_page": 0,
                "type": "Page-footer",
                "bbox": {
                    "xmin": 0.4954006071645416,
                    "ymin": 0.9445,
                    "xmax": 0.503205343047966,
                    "ymax": 0.9531
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "4"
            }
        },
        {
            "extraction_id": 10,
            "metadata": {
                "source_page": 0,
                "type": "Table",
                "bbox": {
                    "xmin": 0.17880850030358225,
                    "ymin": 0.0852,
                    "xmax": 0.8197974499089253,
                    "ymax": 0.3375
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "tabular",
                "content_html": "<table>\n  <tr>\n    <td><b>Query:</b> Could you provide me some studies that proposed hierarchical neural models to capture spatiotemporal features in sign videos?</td>\n  </tr>\n  <tr>\n    <td><b>Query Date:</b> 2023-05-02</td>\n  </tr>\n  <tr>\n    <td><b>Answer Papers:</b></td>\n  </tr>\n  <tr>\n    <td>\\[1\\] TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for Sign Language Translation</td>\n  </tr>\n  <tr>\n    <td>\\[2\\] Sign Language Translation with Hierarchical Spatio-Temporal Graph Neural Network</td>\n  </tr>\n  <tr>\n    <td><b>Source:</b> SLTUnet: A Simple Unified Model for Sign Language Translation, ICLR 2023</td>\n  </tr>\n  <tr>\n    <td><b>Query:</b> Which studies have focused on nonstationary RL using value-based methods, specifically Upper Confidence Bound (UCB) based algorithms?</td>\n  </tr>\n  <tr>\n    <td><b>Query Date:</b> 2023-08-10</td>\n  </tr>\n  <tr>\n    <td><b>Answer Papers:</b></td>\n  </tr>\n  <tr>\n    <td>\\[1\\] Reinforcement Learning for Non-Stationary Markov Decision Processes: The Blessing of (More) Optimism</td>\n  </tr>\n  <tr>\n    <td>\\[2\\] Efficient Learning in Non-Stationary Linear Markov Decision Processes</td>\n  </tr>\n  <tr>\n    <td>\\[3\\] Nonstationary Reinforcement Learning with Linear Function Approximation</td>\n  </tr>\n  <tr>\n    <td><b>Source:</b> Provably Efficient Algorithm for Nonstationary Low-Rank MDPs, NeurIPS 2023</td>\n  </tr>\n  <tr>\n    <td><b>Query:</b> Which studies have been conducted in long-form text generation, specifically in story generation?</td>\n  </tr>\n  <tr>\n    <td><b>Query Date:</b> 2024-01-26</td>\n  </tr>\n  <tr>\n    <td><b>Answer Papers:</b></td>\n  </tr>\n  <tr>\n    <td>\\[1\\] Strategies for Structuring Story Generation</td>\n  </tr>\n  <tr>\n    <td>\\[2\\] MEGATRON-CNTRL: Controllable Story Generation with External Knowledge Using Large-Scale Language Models</td>\n  </tr>\n  <tr>\n    <td><b>Source:</b> ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models, ACL 2024</td>\n  </tr>\n</table>"
            }
        },
        {
            "extraction_id": 11,
            "metadata": {
                "source_page": 0,
                "type": "Table",
                "bbox": {
                    "xmin": 0.16900255009107468,
                    "ymin": 0.3914,
                    "xmax": 0.48169228901032185,
                    "ymax": 0.4664
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "tabular",
                "content_html": "<table>\n  <tr>\n    <td>Conference</td>\n    <td>|<i>P</i>|</td>\n    <td>|<i>Q</i>|</td>\n    <td><i>Ans(/Q</i>)</td>\n    <td><i>Ans</i>-50</td>\n    <td><i>Ans</i>-90</td>\n  </tr>\n  <tr>\n    <td>ICLR 2023</td>\n    <td>888</td>\n    <td>5204</td>\n    <td>2.46</td>\n    <td>2.0</td>\n    <td>5.0</td>\n  </tr>\n  <tr>\n    <td>ICML 2023</td>\n    <td>981</td>\n    <td>5743</td>\n    <td>2.37</td>\n    <td>2.0</td>\n    <td>5.0</td>\n  </tr>\n  <tr>\n    <td>NeurIPS 2023</td>\n    <td>1948</td>\n    <td>11761</td>\n    <td>2.59</td>\n    <td>2.0</td>\n    <td>5.0</td>\n  </tr>\n  <tr>\n    <td>CVPR 2024</td>\n    <td>1336</td>\n    <td>9528</td>\n    <td>2.94</td>\n    <td>2.0</td>\n    <td>6.0</td>\n  </tr>\n  <tr>\n    <td>ACL 2024</td>\n    <td>485</td>\n    <td>3315</td>\n    <td>2.16</td>\n    <td>2.0</td>\n    <td>4.0</td>\n  </tr>\n</table>"
            }
        },
        {
            "extraction_id": 12,
            "metadata": {
                "source_page": 0,
                "type": "Caption",
                "bbox": {
                    "xmin": 0.26876308439587127,
                    "ymin": 0.3523,
                    "xmax": 0.7289423193685489,
                    "ymax": 0.3633
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "1: Examples of queries and corresponding papers in AutoScholarQuery."
            }
        },
        {
            "extraction_id": 13,
            "metadata": {
                "source_page": 0,
                "type": "Caption",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.482,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.5641
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "2: Statistics of AutoScholarQuery. |_P_| and |_Q_| represent the total number of papers and queries collected for each conference. _Ans(/Q_) denotes the average number of answer papers per query. _Ans_-50 and _Ans_-90 refers to the 50th and 90th percentiles of answer paper counts per query."
            }
        }
    ]
}