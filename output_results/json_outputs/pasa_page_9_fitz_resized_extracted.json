{
    "source_document": "output_results/resized_images/pasa_page_9_fitz_resized.png",
    "source_page_number": 0,
    "processing_timestamp_utc": "2025-11-17T21:37:30.578716+00:00",
    "status": "Layout extraction successful (Docker Inference)",
    "content": [
        {
            "extraction_id": 0,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.0883,
                    "xmax": 0.3342027929568913,
                    "ymax": 0.1
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "<tbc>of the entire agent system."
            }
        },
        {
            "extraction_id": 1,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.1055,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.3906
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "As both the final decision-maker and auxiliary reward model in RL training for the Crawler, the performance of the Selector is crucial. To evaluate its effectiveness, we collected a dataset of 200 query-paper pairs, annotating whether each paper meets the query\u2019s requirements. This dataset serves as the benchmark for evaluating the Selector (see Appendix C for details). We then compared our Selector against GPT-4o (Hurst et al., 2024) and Qwen-2.5-7b (Yang et al., 2024), as shown in  7. The results show that our Selector achieves an F1 score of 85%, outperforming GPT-4o by 5% and Qwen-2.5-7b by 30%. Additionally, when compared to a setting where reasoning precedes decision token generation, the performance is comparable. Lastly, the Selector\u2019s precision reaches 95%, confirming its effectiveness as an auxiliary reward model for the Crawler RL training."
            }
        },
        {
            "extraction_id": 2,
            "metadata": {
                "source_page": 0,
                "type": "Section-header",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.5312,
                    "xmax": 0.3019832422586521,
                    "ymax": 0.543
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "### 5.4 Ablation study"
            }
        },
        {
            "extraction_id": 3,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.5539,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.807
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "We perform ablation studies in  8 to evaluate the individual contributions of exploring citation networks, RL training, and using the Selector as the reward model. The results indicate that removing the `\\[Expand\\]` action from the Crawler leads to a significant drop in the recall: a decrease of 22.98% on AutoScholarQuery and 32.21% on RealScholarQuery. Furthermore, RL training enhances recall by 6.24% on AutoScholarQuery and 19.96% on RealScholarQuery. The RL training curves are depicted in Figure 3, where the training curves show a steady increase in return with the training steps, eventually converging after 200 steps. Finally, removing the Selector as an auxiliary reward model results in a 3.76% recall drop on AutoScholarQuery and a 9.63% drop on RealScholarQuery."
            }
        },
        {
            "extraction_id": 4,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.8117,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.9203
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "We investigate how to control agent behavior by adjusting the rewards in RL training. Experiments are conducted with varying reward coefficients \\(\\alpha\\) in Equation 1, and results are presented in  9. We report two metrics: crawler recall and crawler action. The crawler action refers to the total number of `\\[Search\\]` and `\\[Expand\\]` actions throughout the<tbc>"
            }
        },
        {
            "extraction_id": 5,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.0883,
                    "xmax": 0.8374081360048573,
                    "ymax": 0.1477
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "<tbc>Crawler\u2019s entire trajectory. As the reward increases, both crawler recall and crawler action increase, suggesting that adjusting rewards in RL training can effectively influence PaSa\u2019s behavior."
            }
        },
        {
            "extraction_id": 6,
            "metadata": {
                "source_page": 0,
                "type": "Section-header",
                "bbox": {
                    "xmin": 0.512010686095932,
                    "ymin": 0.5422,
                    "xmax": 0.6224777170613236,
                    "ymax": 0.5516
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "## 6 Conclusion"
            }
        },
        {
            "extraction_id": 7,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.5711,
                    "xmax": 0.8374081360048573,
                    "ymax": 0.9047
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "In this paper, we introduce PaSa, a novel paper search agent designed to provide comprehensive and accurate results for complex academic queries. PaSa is implemented within the AGILE, a reinforcement learning framework for LLM agents. To train PaSa, we developed AutoScholarQuery, a dataset of fine-grained academic queries and corresponding papers drawn from top-tier AI conference publications. To evaluate PaSa in real-world scenarios, we also constructed RealScholarQuery, a dataset of actual academic queries paired with annotated papers. Our experimental results demonstrate that PaSa outperforms all baselines, including Google, Google Scholar, and Google with GPT-4o, ChatGPT, GPT-o1, and PaSa-GPT-4o. In particular, PaSa-7B surpasses Google with GPT-4o by 37.78% in recall@20 and 39.90% in recall@50, while also exceeding PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. These findings underscore PaSa significantly improves the efficiency and accuracy of academic search."
            }
        },
        {
            "extraction_id": 8,
            "metadata": {
                "source_page": 0,
                "type": "Page-footer",
                "bbox": {
                    "xmin": 0.4954006071645416,
                    "ymin": 0.9445,
                    "xmax": 0.503205343047966,
                    "ymax": 0.9531
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "9"
            }
        },
        {
            "extraction_id": 9,
            "metadata": {
                "source_page": 0,
                "type": "Table",
                "bbox": {
                    "xmin": 0.17100376442015786,
                    "ymin": 0.4109,
                    "xmax": 0.4797911353976928,
                    "ymax": 0.4758
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "tabular",
                "content_html": "<table>\n  <tr>\n    <td><b>Method</b></td>\n    <td><b>Precision</b></td>\n    <td><b>Recall</b></td>\n    <td><b>F1</b></td>\n  </tr>\n  <tr>\n    <td>GPT-4o</td>\n    <td>0.96</td>\n    <td>0.69</td>\n    <td>0.80</td>\n  </tr>\n  <tr>\n    <td>Qwen-2.5-7b</td>\n    <td>1.0</td>\n    <td>0.38</td>\n    <td>0.55</td>\n  </tr>\n  <tr>\n    <td>PaSa-7b-Selector</td>\n    <td>0.95</td>\n    <td>0.78</td>\n    <td>0.85</td>\n  </tr>\n  <tr>\n    <td>PaSa-7b-Selector (Reason First)</td>\n    <td>0.94</td>\n    <td>0.76</td>\n    <td>0.84</td>\n  </tr>\n</table>"
            }
        },
        {
            "extraction_id": 10,
            "metadata": {
                "source_page": 0,
                "type": "Picture",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.1672,
                    "xmax": 0.8374081360048573,
                    "ymax": 0.425
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {}
        },
        {
            "extraction_id": 11,
            "metadata": {
                "source_page": 0,
                "type": "Caption",
                "bbox": {
                    "xmin": 0.23944529447480267,
                    "ymin": 0.4922,
                    "xmax": 0.41044905889496053,
                    "ymax": 0.5
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "7: Selector Evaluation."
            }
        },
        {
            "extraction_id": 12,
            "metadata": {
                "source_page": 0,
                "type": "Caption",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.4375,
                    "xmax": 0.8364075288403158,
                    "ymax": 0.5047
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "Figure 3: Return and value function loss curves during the PPO training process. The smoothing method of the curve in the figures is the exponential moving average(EMA) formula that aligns with the one used in TensorBoard, and the smoothing weight is set to 0.95."
            }
        }
    ]
}