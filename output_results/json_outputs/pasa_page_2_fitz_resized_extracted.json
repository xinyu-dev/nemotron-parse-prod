{
    "source_document": "output_results/resized_images/pasa_page_2_fitz_resized.png",
    "source_page_number": 0,
    "processing_timestamp_utc": "2025-11-17T21:37:06.959414+00:00",
    "status": "Layout extraction successful (Docker Inference)",
    "content": [
        {
            "extraction_id": 0,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.3516,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.6203
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "<tbc>In this paper, we introduce PaSa, a novel paper search agent designed to mimic human behavior for comprehensive and accurate academic paper searches. As illustrated in Figure 1, PaSa consists of two LLM agents: the Crawler and the Selector. For a given user query, the Crawler can autonomously collect relevant papers by utilizing search tools or extracting citations from the current paper, which are then added to a growing _paper queue_. The Crawler iteratively processes each paper in the paper queue, navigating citation networks to discover increasingly relevant papers. The Selector carefully reads each paper in the paper queue to determine whether it meets the requirements of the user query. We optimize PaSa within the AGILE, a reinforcement learning (RL) framework for LLM agents (Feng et al., 2024)."
            }
        },
        {
            "extraction_id": 1,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.625,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.8461
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "Effective training requires high-quality academic search data. Fortunately, human scientists have already created a vast amount of high-quality academic papers, which contain extensive surveys on a wide range of research topics. We build a synthetic but high-quality academic search dataset, AutoScholarQuery, which collects fine-grained scholar queries and their corresponding relevant papers from the related work sections of papers published at ICLR 2023 <sup>1</sup>, ICML 2023 <sup>2</sup>, NeurIPS 2023 <sup>3</sup>, ACL 2024 <sup>4</sup>, and CVPR 2024 <sup>5</sup>. AutoScholarQuery includes 33,511 / 1,000 / 1,000 query-paper pairs in the training / development / test split."
            }
        },
        {
            "extraction_id": 2,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.3516,
                    "xmax": 0.8364075288403158,
                    "ymax": 0.5727
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "Although AutoScholarQuery only provides query and paper answers, without demonstrating the path by which scientists collect the papers, we can utilize them to perform RL training to improve PaSa. In addition, we design a new session-level PPO (Proximal Policy Optimization (Schulman et al., 2017)) training method to address the unique challenges of the paper search task: 1) sparse reward: The papers in AutoScholarQuery are collected via citations, making it a smaller subset of the actual qualified paper set. 2) long trajectories: The complete trajectory of the Crawler may involve hundreds of papers, which is too long to directly input into the LLM context."
            }
        },
        {
            "extraction_id": 3,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.5867,
                    "xmax": 0.8364075288403158,
                    "ymax": 0.9203
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "To evaluate PaSa, besides the test set of AutoScholarQuery, we also develop a benchmark, RealScholarQuery. It contains 50 real-world academic queries with annotated relevant papers, to assess PaSa in real-world scenarios. We compare PaSa with several baselines including Google, Google Scholar, Google paired with GPT-40 for paraphrased queries, chatGPT (search-enabled GPT- 4o), GPT-o1 and PaSa-GPT-4o (PaSa agent realized by prompting GPT-4o). Our experiments show that PaSa-7b significantly outperforms all baselines. Specifically, for AutoScholarQuery test set, PaSa- 7b achieves a 34.05% improvement in Recall@20 and a 39.36% improvement in Recall@50 compared to Google with GPT-4o, the strongest Google- based baseline. PaSa-7b surpasses PaSa-GPT-4o by 11.12% in recall, with similar precision. For RealScholarQuery, PaSa-7b outperforms Google with GPT-4o by 37.78% in Recall@20 and 39.90% in Recall@50. PaSa-7b surpasses PaSa-GPT-4o by 30.36% in recall and 4.25% in precision."
            }
        },
        {
            "extraction_id": 4,
            "metadata": {
                "source_page": 0,
                "type": "Footnote",
                "bbox": {
                    "xmin": 0.18271086824529448,
                    "ymin": 0.857,
                    "xmax": 0.4533751062537948,
                    "ymax": 0.9195
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "<sup>1</sup>https://iclr.cc/Conferences/2023 <sup>2</sup>https://icml.cc/Conferences/2023 <sup>3</sup>https://neurips.cc/Conferences/2023 <sup>4</sup>https://2024.aclweb.org/ <sup>5</sup>https://cvpr.thecvf.com/Conferences/2024"
            }
        },
        {
            "extraction_id": 5,
            "metadata": {
                "source_page": 0,
                "type": "Page-footer",
                "bbox": {
                    "xmin": 0.4954006071645416,
                    "ymin": 0.9445,
                    "xmax": 0.503205343047966,
                    "ymax": 0.9531
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "2"
            }
        },
        {
            "extraction_id": 6,
            "metadata": {
                "source_page": 0,
                "type": "Picture",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.0852,
                    "xmax": 0.9194579234972678,
                    "ymax": 0.2391
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {}
        },
        {
            "extraction_id": 7,
            "metadata": {
                "source_page": 0,
                "type": "Caption",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.2547,
                    "xmax": 0.8355069823922282,
                    "ymax": 0.3219
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "Figure 1: Architecture of PaSa. The system consists of two LLM agents, Crawler and Selector. The Crawler processes the user query and can access papers from the paper queue. It can autonomously invoke the search tool, expand citations, or stop processing of the current paper. All papers collected by the Crawler are appended to the paper queue. The Selector reads each paper in the paper queue to determine whether it meets the criteria specified in the user query."
            }
        }
    ]
}