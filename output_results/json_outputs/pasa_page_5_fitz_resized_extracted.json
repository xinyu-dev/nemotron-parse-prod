{
    "source_document": "output_results/resized_images/pasa_page_5_fitz_resized.png",
    "source_page_number": 0,
    "processing_timestamp_utc": "2025-11-17T21:37:16.379690+00:00",
    "status": "Layout extraction successful (Docker Inference)",
    "content": [
        {
            "extraction_id": 0,
            "metadata": {
                "source_page": 0,
                "type": "Section-header",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.5945,
                    "xmax": 0.25795652701882205,
                    "ymax": 0.6039
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "## 4.2 Crawler"
            }
        },
        {
            "extraction_id": 1,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.618,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.7883
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "In RL terminology, the Crawler performs a token- level Markov Decision Process (MDP). The action space A corresponds to the LLM\u2019s vocabulary, where each token represents an action. The LLM functions as the policy model. The agent\u2019s state is defined by the current LLM context and the paper queue. The Crawler operates with three registered functions, as outlined in  3. When an action matches a function name, the corresponding function is executed, further modifying the agent\u2019s state."
            }
        },
        {
            "extraction_id": 2,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.7961,
                    "xmax": 0.4895970856102004,
                    "ymax": 0.9203
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "For example, as Figure 2 shows, the agent begins by receiving a user query, incorporating it into its context, and initiating actions. If the token generated is \\[Search\\], the LLM continues to generate a search query, and the agent invokes a search tool to retrieve papers, which are then added to the paper list. If the token is \\[Expand\\], the LLM continues to extract a subsection name from the current pa<tbc>"
            }
        },
        {
            "extraction_id": 3,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.512010686095932,
                    "ymin": 0.4062,
                    "xmax": 0.8345063752276867,
                    "ymax": 0.5273
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "<tbc>per in its context. The agent subsequently uses a parsing tool to extract all referenced papers within that subsection, adding them to the paper list. If the token is \\[Stop\\], the agent resets its context to the user query and information of the next paper in the paper queue. This information includes the title, abstract, and an outline of all sections and subsections."
            }
        },
        {
            "extraction_id": 4,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.512010686095932,
                    "ymin": 0.5359,
                    "xmax": 0.8364075288403158,
                    "ymax": 0.6438
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "The training process for the Crawler comprises two stages. In the first stage, we generate trajectories for a small subset of the training data and then perform imitation learning (see Appendix A.1 for details). In the second stage, reinforcement learning is applied. The details of the RL training implementation are described below."
            }
        },
        {
            "extraction_id": 5,
            "metadata": {
                "source_page": 0,
                "type": "Section-header",
                "bbox": {
                    "xmin": 0.512010686095932,
                    "ymin": 0.6602,
                    "xmax": 0.6175747419550698,
                    "ymax": 0.6719
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "### Reward Design"
            }
        },
        {
            "extraction_id": 6,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.5110100789313905,
                    "ymin": 0.6602,
                    "xmax": 0.8364075288403158,
                    "ymax": 0.8008
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "We conduct RL training on the AutoScholarQuery training set, where each instance consists of a query _q_ and a corresponding paper set P. Starting with a query _q_, the Crawler generates a trajectory \\(\\tau=(s_1,a_1,\\cdots,s_T,a_T)\\). At each time step _t_, we denote the current paper queue as Q_<sub>t</sub>_. Upon taking action _a<sub>t</sub>_, the Crawler appends a set of new papers \\((p_1,p_2,\\cdots,p_{n_t})\\) to the paper queue. If \\(a_t=\\text{\\texttt{[Stop}}]\\), the set is empty."
            }
        },
        {
            "extraction_id": 7,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.512010686095932,
                    "ymin": 0.8063,
                    "xmax": 0.8345063752276867,
                    "ymax": 0.8313
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "The reward of executing action _a<sub>t</sub>_ in state _s<sub>t</sub>_ is defined as"
            }
        },
        {
            "extraction_id": 8,
            "metadata": {
                "source_page": 0,
                "type": "Formula",
                "bbox": {
                    "xmin": 0.566743897996357,
                    "ymin": 0.8492,
                    "xmax": 0.8355069823922282,
                    "ymax": 0.8781
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "\\(r(s_t,a_t)=\\alpha\\times\\sum_{i=1}^{n_t}\\mathbb{I}(q,p_i,t)-c(a_t),\\) (1)"
            }
        },
        {
            "extraction_id": 9,
            "metadata": {
                "source_page": 0,
                "type": "Text",
                "bbox": {
                    "xmin": 0.512010686095932,
                    "ymin": 0.8922,
                    "xmax": 0.8374081360048573,
                    "ymax": 0.9203
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "where I(_q,p<sub>i</sub>,t_)=1 if _p<sub>i</sub>_ matches the query _q_ and is not already in Q_<sub>t</sub>_, and I(_q,p<sub>i</sub>,t_)=0 otherwise.<tbc>"
            }
        },
        {
            "extraction_id": 10,
            "metadata": {
                "source_page": 0,
                "type": "Page-footer",
                "bbox": {
                    "xmin": 0.4954006071645416,
                    "ymin": 0.9445,
                    "xmax": 0.503205343047966,
                    "ymax": 0.9531
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "5"
            }
        },
        {
            "extraction_id": 11,
            "metadata": {
                "source_page": 0,
                "type": "Table",
                "bbox": {
                    "xmin": 0.16810200364298727,
                    "ymin": 0.3953,
                    "xmax": 0.4826928961748634,
                    "ymax": 0.5422
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "tabular",
                "content_html": "<table>\n  <tr>\n    <td><b>Name</b></td>\n    <td><b>Implementation</b></td>\n  </tr>\n  <tr>\n    <td>\\[Search\\]</td>\n    <td>Generate a search query and invoke\n\nthe search tool. Append all resulting\n\npers to the paper queue.</td>\n  </tr>\n  <tr>\n    <td>\\[Expand\\]</td>\n    <td>Generate a subsection name, then\n\nadd all referenced papers in the sub-\n\nsection to the paper queue.</td>\n  </tr>\n  <tr>\n    <td>\\[Stop\\]</td>\n    <td>Reset the context to the user query and\n\nthe next paper in the paper queue.</td>\n  </tr>\n</table>"
            }
        },
        {
            "extraction_id": 12,
            "metadata": {
                "source_page": 0,
                "type": "Picture",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.0891,
                    "xmax": 0.8345063752276867,
                    "ymax": 0.3102
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {}
        },
        {
            "extraction_id": 13,
            "metadata": {
                "source_page": 0,
                "type": "Caption",
                "bbox": {
                    "xmin": 0.16419963570127505,
                    "ymin": 0.3234,
                    "xmax": 0.8345063752276867,
                    "ymax": 0.3766
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "Figure 2: An example of the PaSa workflow. The Crawler runs multiple \\[Search\\] using diverse and complementary queries. In addition, the Crawler can evaluate the long-term value of its actions. Notably, it discovers many relevant papers as it explores deeper on the citation network, even when intermediate papers along the path do not align with the user query."
            }
        },
        {
            "extraction_id": 14,
            "metadata": {
                "source_page": 0,
                "type": "Caption",
                "bbox": {
                    "xmin": 0.22473636915604128,
                    "ymin": 0.5602,
                    "xmax": 0.4260585306618093,
                    "ymax": 0.568
                }
            },
            "confidence": "N/A",
            "extraction_status": "Success",
            "data": {
                "type": "textual",
                "content": "3: Functions of the Crawler."
            }
        }
    ]
}